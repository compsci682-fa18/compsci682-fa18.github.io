<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- HTML4 meta tags forcing no-caching -->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
    <meta http-equiv="Pragma" content="no-cache" />
    <meta http-equiv="Expires" content="0" />

    <link rel="icon" href="/favicon.ico?">

    <title>COMPSCI 682 Neural Networks: A Modern Introduction</title>

    <!-- Bootstrap core CSS -->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="assets/css/offcanvas.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="assets/js/ie-emulation-modes-warning.js"></script>

<!--     HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--
    [if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
     [endif]
-->
  </head>
  <body>
    <nav class="navbar navbar-fixed-top navbar-custom">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
<!--          <a class="navbar-brand" href="https://www.cics.umass.edu/">COMPSCI697L</a>-->
          <a class="navbar-brand" href="https://www.cics.umass.edu/"><img style="max-height:15px; margin-top: 1px;"
             src="/assets/fig/umasslogo2.png"></a>
<!--          <a class="navbar-brand" href="#">COMPSCI697L</a>-->
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="/index.html">Home</a></li>
            <li class="active"><a href="/syllabus.html">Syllabus</a></li>
            <li><a href="/notes/">Notes</a></li>
            <li><a href="/assignments.html">Assignments</a></li>
            <li><a href="/projects/">Project</a></li>
          </ul>
        </div><!-- /.nav-collapse -->
      </div><!-- /.container -->
    </nav><!-- /.navbar -->

    <div class="container">
<h2>COMPSCI 682 Neural Networks: A Modern Introduction</h2>
      <div class="panel panel-info">
        <div class="panel-heading">
          <h3 class="panel-title">Note</h3>
        </div>
        <div class="panel-body">
          <ul>
            <li>This is a tentative class outline and is subject to change throughout the semester. </li>
            <li>Slides will be finalized after each lecture.</li>
          </ul>
        </div>
      </div>
<div class="panel panel-default">
<table class="table">
  <tbody><tr class="active">
    <th>Event Type</th><th class="footer-col-4">Date</th><th class="footer-col-3">Description</th><th>Course Materials</th>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Sept. 4</td>
    <td>Intro to Deep Learning, historical context.</td>
    <td>
<!--      <a href="https://docs.google.com/presentation/d/1NceEtoS8p8sjZuGjode_z4shWzhbq-wGtx2BrTOX0bg/edit?usp=sharing">[slides]</a><br>-->
        <a href="https://docs.google.com/presentation/d/1FZOgqMlfDQgu-PXARvRA-TfNjLDQbffIhMvYTgJJkCo/edit?usp=sharing">[slides]</a><br>                
      <a href="notes/python-numpy-tutorial/">[python/numpy tutorial]</a><br>
      <a href="notes/jupyter-tutorial/">[jupyter tutorial]</a>
    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Thursday, Sept. 6</td>
    <td>Image classification and the data-driven approach <br> k-nearest neighbor <br> Linear classification</td>
    <td>
<!--      <a href="https://docs.google.com/presentation/d/1xyJYxYlfUh4LGCheTbPpw74AKpABPSKj6VT0JO4_om8/edit?usp=sharing">[slides]</a><br>-->
        <a href="https://docs.google.com/presentation/d/16K7QYIR5ZU_0voWYzba-r_gMKwTyiQa8owSGa1gKx_s/edit?usp=sharing">[slides]</a><br>        
      <a href="notes/classification/">[image classification notes]</a><br>
      <a href="notes/linear-classify/">[linear classification notes]</a><br>
    </td>
  </tr>
  <tr class="info">
    <td>Optional Discussion</td>
    <td>Friday, Sept. 7</td>
    <td>No discussion section</td>
    <td>
    </td>
  </tr>      
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Sept. 11</td>
    <td>
        Loss functions
    </td>
    <td>
<!--      <a href="https://docs.google.com/presentation/d/1YFd638UtoRJLZBeIMaQR_Kr87lavvdU_WDJQSt246as/edit?usp=sharing">[slides]</a><br>-->
      <a href="https://docs.google.com/presentation/d/1NKuP30UWLvPGdNH4ky8mVKap7P8-vEFXFqF6Bjk1NvQ/edit?usp=sharing">[slides]</a><br>        
    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Thursday, Sept. 13</td>
    <td>
        Optimization: Stochastic Gradient Descent and Backpropagation
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1KD-VvcLioXZyT7Nk2MwDTYveSHSFrAdlhYifrMW41mQ/edit?usp=sharing">[slides]</a><br>
      <a href="notes/optimization-1/">[optimization notes]</a>
    </td>
  </tr>
<!--
  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Feb. 2</td>
    <td>Slicing and broadcasting in Python</td>
    <td>
    </td>
  </tr>
-->
  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Sept. 14</td>
    <td>(9:05-9:55am CS140) Slicing and broadcasting in Python</td>
    <td>
    <a href="notes/slicing_broadcasting_09142018.ipynb">[slicing and broadcasting ipynb]</a><br>
    </td>
  </tr>      
      
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Sept. 18</td>
    <td>
        Backpropagation &amp; Neural Networks I
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/17lZFXaN4fS-XS6pBvOtEmmlOfgXGsFRf7HsiItPvjK8/edit?usp=sharing">[slides]</a><br>
      <a href="notes/optimization-2/">[backprop notes]</a><br>
      <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">[Efficient BackProp]</a> (optional)<br>
      related: <a href="http://colah.github.io/posts/2015-08-Backprop/">[1]</a>, <a href="http://neuralnetworksanddeeplearning.com/chap2.html">[2]</a>, <a href="https://www.youtube.com/watch?v=q0pm3BrIUFo">[3]</a> (optional)
    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Thursday, Sept. 20</td>
    <td>
        Neural Networks II<br>
        Higher-level representations, image features<br>
        Vector, Matrix, and Tensor Derivatives
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1klIUc7Y2B_gM2ab1slsacP1EWZdQ_oxm6_6hJLWf8NA/edit?usp=sharing">[slides]</a><br>
      handout 1: <a href="/docs/vecDerivs.pdf">Vector, Matrix, and Tensor Derivatives</a><br>
      handout 2: <a href="http://cs231n.stanford.edu/handouts/derivatives.pdf">Derivatives, Backpropagation, and Vectorization</a><br>
      <a href="http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html">Deep Learning [Nature]</a> (optional)
    </td>
  </tr>      
  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Sept. 21</td>
    <td>(11:05-12:05 CS142) Vector, Matrix, and Tensor Derivatives</td>
    <td><a href="https://compsci682-fa18.github.io/docs/vecDerivs.pdf">[notes]</a></td>
  </tr>
      <!--
  <tr class="success">
    <td>Optional Discussion</td>
    <td>TBD</td>
    <td>Reviewing the chain rule, applying the chain rule to vectors</td>
    <td>
    </td>
  </tr>
-->
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Sept. 25</td>
    <td>
        Neural Networks III
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1dyM5-8E1ZrPhI6SxLYAg42R31W7MiK6HylKZT5M6cl0/edit?usp=sharing">[slides]</a><br> 
        tips/tricks:
        <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">[1]</a>,
        <a href="http://arxiv.org/pdf/1206.5533v2.pdf">[2]</a> (optional)
        <br>
    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Thursday, Sept. 27</td>
    <td>
        Training Neural Networks I: <br>
        Activation Functions
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1qQ592vyLnJeb4NQKX5yLzs1xqsT2T5Ap4qNxYq_bVGQ/edit?usp=sharing">[slides]</a><br> 
      <a href="notes/neural-networks-1/">[Neural Nets notes 1]</a>
    </td>
  </tr>      
  <tr class="info">
    <td>Optional Discussion</td>
    <td>Friday, Sept. 21</td>
    <td>No discussion section</td>
    <td></td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Oct. 2</td>
    <td>
        Training Neural Networks II: <br>
        weight initialization, batch normalization
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1mtRA5rW63LEA0OVGIKCoGl-vEJz1OQvY7uuD1kjiDu0/edit?usp=sharing">[slides]</a><br> 
        <a href="notes/neural-networks-2/">[Neural Nets notes 2]</a><br> 
        <a href="https://arxiv.org/abs/1502.03167">[Batch Norm]</a><br>
        <a href="https://drive.google.com/file/d/0B-0OtUj_Gj7nY0pzQWhoZnVVWDQ/view?usp=sharing">Copula Normalization</a> (optional)
    </td>
  </tr>      
 <tr>
    <td>Lecture</td>
    <td>Thursday, Oct. 4</td>
    <td>
        Training Neural Network III: <br>
        babysitting the learning process, hyperparameter optimization
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1j0zLaL58Hqo3YS9Di3CZR_qnxlGIdlEqOQU0-uTD2j0/edit?usp=sharing">[slides]</a><br> 
        <a href="https://arxiv.org/abs/1206.5533">[Bengio 2012]</a> (optional)
    </td>
  </tr>  
  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Oct. 5</td>
    <td>(11:05-12:05 CS142) Google Cloud, PyTorch, Tensorflow tutorials</td>
    <td><a href="https://compsci682-fa18.github.io/gce-tutorial">[Google Cloud Tutorial]</a><br>
    <a href="http://cs231n.stanford.edu/notebooks/pytorch_tutorial.ipynb">[PyTorch Notebook (from Stanford)]</a></td>
  </tr>

  <tr class="info">
    <td>No Class (Monday Schedule) </td>
    <td>Tuesday, Oct. 9</td>
    <td>Monday class schedule will be followed</td>
    <td>
    </td>
  </tr>
      
  <tr>
    <td>Lecture</td>
    <td>Thursday, Oct. 11</td>
    <td>
        Training Neural Network III: <br>
        babysitting the learning process, hyperparameter optimization
        Training Neural Network IV: <br>
        model ensembles, dropout
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1j0zLaL58Hqo3YS9Di3CZR_qnxlGIdlEqOQU0-uTD2j0/edit?usp=sharing">[slides]</a><br> 
        <a href="https://docs.google.com/presentation/d/1ek_qy3b0N6xBv4rJ3OGsEuZ-5ZgbQ1FfQE3fqiYVG-U/edit?usp=sharing">[slides]</a><br>
        <a href="notes/neural-networks-3/">[Neural Nets notes 3]</a><br>
        <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">LeNet</a> (optional)<br>
    </td>
  </tr>   


  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Oct. 12</td>
    <td>(11:05-12:05 CS142) A closer look at the maths inside batch normalization</td>
    <td>
    </td>
  </tr>
   
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Oct. 16</td>
    <td>
        Training Neural Network V: <br>
        parameter updates<br>
        Convolutional Neural Networks: <br>
        convolution layer, pooling layer, fully connected layer <br>
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1wA0TwJ_Gd4rIwelljTvcVdc5gCUi5taKBPl2PZkUXME/edit?usp=sharing">[slides]</a><br>
        <a href="https://docs.google.com/presentation/d/1iVrs2ut8OR_NH8pte-uwXeX5JzOhOMK-DBGG2JzKTCI/edit?usp=sharing">[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture</td>
    <td>Thursday, Oct. 18</td>
    <td>
        Convolutional Neural Networks: (cont.)<br>
        convolution layer, pooling layer, fully connected layer <br>
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1iVrs2ut8OR_NH8pte-uwXeX5JzOhOMK-DBGG2JzKTCI/edit?usp=sharing">[slides]</a>
    </td>  </tr>

  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Oct. 19</td>
    <td>Convolutional neural networks</td>
    <td>
      <a href="docs/conv2d_discuss.pdf">[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture</td>
    <td>Tuesday, Oct. 23</td>
    <td>
        Geust Lecture: <a href="https://people.cs.umass.edu/~smaji/">Subhransu Maji</a> (topic TBD)<br>
    </td>
    <td>
        
    </td>

  </tr>

  <tr>
    <td>Lecture</td>
    <td>Thursday, Oct. 25</td>
    <td>ConvNets for spatial localization, Object detection<br>
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1l75Lpv8caesyvYWWfo0gfnZHGPuwQdjxf8x1oOSiAm8/edit?usp=sharing">[slides]</a><br>
      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">FCN</a>
    </td>
  </tr>

  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Oct. 26</td>
    <td>TBD</td>
    <td>
    </td>
  </tr>

  <tr>
    <td>Lecture</td>
    <td>Tuesday, Oct. 30</td>
    <td>
        ConvNets for spatial localization, Object detection (cont.)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1l75Lpv8caesyvYWWfo0gfnZHGPuwQdjxf8x1oOSiAm8/edit?usp=sharing">[slides]</a><br>
    </td>
  </tr>

  <tr>
    <td>Lecture</td>
    <td>Thursday, Nov. 1</td>
    <td>
      ConvNets for spatial localization, Object detection (cont.)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1l75Lpv8caesyvYWWfo0gfnZHGPuwQdjxf8x1oOSiAm8/edit?usp=sharing">[slides]</a>
      <!--
        <a href="https://docs.google.com/presentation/d/17P1pFTG7seo6EuDhLrJDI6Bb7ce1_mUd28qhJKe5j_k/edit?usp=sharing">[slides]</a><br>
        <a href="https://arxiv.org/abs/1512.03385">ResNet</a> (optional)
      -->
    </td>
  </tr>

  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Nov. 2</td>
    <td>
      Understanding and visualizing Convolutional Neural Networks<br>
    </td>
    <td>
      <!--
        <a href="https://docs.google.com/presentation/d/17P1pFTG7seo6EuDhLrJDI6Bb7ce1_mUd28qhJKe5j_k/edit?usp=sharing">[slides]</a> (cont.)<br>
        <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">FCN</a> (optional)
      -->
    </td>
  </tr>

  <tr>
    <td>Lecture</td>
    <td>Tuesday, Nov. 6</td>
    <td>
      Understanding and visualizing Convolutional Neural Networks (cont.)<br>
      Backprop into image: Visualizations, deep dream (cont.)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1xjaze2-G6fZ_9TkLxoK_3p64izDegBbQDMSk7_TN3tI/edit?usp=sharing">[slides]</a><br>
      <a href="notes/understanding-cnn/">[visualization notes]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture</td>
    <td>Thursday, Nov. 8</td>
    <td>
      Artistic style transfer<br>
      Adversarial fooling examples<br>
      Recurrent Neural Networks (RNN)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1DcHsDnG-XiyjuvLcHWRR2sMnBzwBMlJ7nV8Bvq5Nh50/edit?usp=sharing">[slides]</a><br>
      <a href="http://www.deeplearningbook.org/contents/rnn.html">DL book RNN chapter</a> (optional)<br>
      <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">min-char-rnn</a>, <a href="https://github.com/karpathy/char-rnn">char-rnn</a>, <a href="https://github.com/karpathy/neuraltalk2">neuraltalk2</a>
    </td>
  </tr>

  <tr class="success">
    <td>Optional Discussion</td>
    <td>Friday, Nov. 9</td>
    <td>Midterm Review</td>
    <td>
    </td>
  </tr>

  <tr>
    <td>Guest Lecture</td>
    <td>Tuesday, Nov. 13</td>
    <td>
        Geust Lecture: <a href="https://people.cs.umass.edu/~strubell/">Emma Strubell</a>, An Introduction to Neural Networks for Natural Language Processing<br>
    </td>
    <td>
    </td>
  </tr>

<tr class="danger">
    <td>Midterm </td>
    <td>Thursday, Nov. 15</td>
    <td>
       In-class midterm
    </td>
    <td>
    </td>
  </tr> 

<!--
  <tr>
    <td>Lecture</td>
    <td>Thursday, Oct. 18</td>
    <td>
        Convolutional Neural Networks: (cont.)<br>
        convolution layer, pooling layer, fully connected layer <br>
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1mIVVop6WGi8BNFet-ajnrf7r4Z-VfyX8XCdyzuM7x1I/edit?usp=sharing">[slides]</a>
    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Oct. 23</td>
    <td>
        ConvNets for spatial localization, Object detection<br>
        Final project information
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/17P1pFTG7seo6EuDhLrJDI6Bb7ce1_mUd28qhJKe5j_k/edit?usp=sharing">[slides]</a><br>
        <a href="https://docs.google.com/presentation/d/1WBhc8HbVn3OV5QpE7jGEV03LQN7mdA747l17WCFpbOQ/edit?usp=sharing">[slides]</a><br>
        <a href="http://cs231n.stanford.edu/reports.html">[Stanford cs231n project reports]</a><br>
        <a href="https://docs.google.com/spreadsheets/d/10rbnBZGXzfDGnTRM8h4Q660ZZ731RSQosZr-RqtlUhU/edit?usp=sharing">[2016 Fall project reports]</a><br>
    </td>
  </tr>      
  <tr >
    <td>Lecture</td>
    <td>Thursday, Oct. 25</td>
    <td>ConvNets for spatial localization, Object detection<br>
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/17P1pFTG7seo6EuDhLrJDI6Bb7ce1_mUd28qhJKe5j_k/edit?usp=sharing">[slides]</a><br>
    </td>
  </tr>
-->
<!--
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Oct. 30</td>
    <td>
        ConvNets for spatial localization, Object detection (cont.)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/17P1pFTG7seo6EuDhLrJDI6Bb7ce1_mUd28qhJKe5j_k/edit?usp=sharing">[slides]</a><br>
      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">FCN</a>
    </td>
  </tr>      
  <tr class="danger">
    <td>Midterm </td>
    <td>Thursday, Nov. 1 (tentative)</td>
    <td>
	     In-class midterm
    </td>
    <td>
    </td>
  </tr>   
-->
<!--
  <tr>
    <td>Lecture</td>
    <td>Tuesday, No. 6</td>
    <td>
      ConvNets for spatial localization, Object detection (cont.)
    </td>
    <td>

      </td>
  </tr>      
-->
<!--
  <tr>
    <td>Lecture</td>
    <td>Thursday, Nov. 8</td>
    <td>
      Understanding and visualizing Convolutional Neural Networks<br>
      Backprop into image: Visualizations, deep dream
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/16VDgX6BwCkJvsJOxKAcjFFbnfWb3A2069gyGOwHWSXY/edit?usp=sharing">[slides]</a><br>
      <a href="notes/understanding-cnn/">[visualization notes]</a>
    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Nov. 13</td>
    <td>
      Understanding and visualizing Convolutional Neural Networks (cont.)<br>
      Backprop into image: Visualizations, deep dream (cont.)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/16VDgX6BwCkJvsJOxKAcjFFbnfWb3A2069gyGOwHWSXY/edit?usp=sharing">[slides]</a><br>
      <a href="notes/understanding-cnn/">[visualization notes]</a>
    </td>
  </tr>     
-->
<!--
  <tr>
    <td>Lecture</td>
    <td>Thursday, Nov. 15</td>
    <td>
      Artistic style transfer<br>
      Adversarial fooling examples<br>
      Recurrent Neural Networks (RNN)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1tv3BCnd8hGk4O_1Rmi4qx2AWinDdYM4dxyS-FMlDlZI/edit?usp=sharing">[slides]</a><br>
      <a href="http://www.deeplearningbook.org/contents/rnn.html">DL book RNN chapter</a> (optional)<br>
      <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">min-char-rnn</a>, <a href="https://github.com/karpathy/char-rnn">char-rnn</a>, <a href="https://github.com/karpathy/neuraltalk2">neuraltalk2</a>
    </td>
  </tr>      
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Nov. 27</td>
    <td>
      Recurrent Neural Networks (RNN) (cont.) <br>
      Long Short Term Memory (LSTM)
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1tv3BCnd8hGk4O_1Rmi4qx2AWinDdYM4dxyS-FMlDlZI/edit?usp=sharing">[slides]</a> (cont.)<br>
      <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of RNN</a> (optional) <br>
      <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (optional)
    </td>
  </tr>      
  <tr>
    <td>Lecture</td>
    <td>Thursday, Nov. 29</td>
    <td>
      Training ConvNets in practice
    </td>
    <td>
      <a href="https://docs.google.com/presentation/d/1mMXpk2DVgTqoArOyvm35U-DnW7-X5gCqXZIqhyH4up4/edit?usp=sharing">[slides]</a><br>
    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Dec. 4</td>
    <td>
      TBD
    </td>
    <td>

    </td>
  </tr>
  <tr>
    <td>Lecture</td>
    <td>Thursday, Dec. 6</td>
    <td>
      TBD
    </td>

  </tr>      
-->
<!--
  <tr class="danger">
    <td>Presentation (tentative) </td>
    <td>Tuesday, Dec. 11</td>
    <td>
      Poster presentations<br>
      Two sessions, both at CS150/151:<br>
      8-10am (regular time)<br>
      12-2:30pm
    </td>
    <td>
    </td>
  </tr>      
-->
<!--
  <tr>
    <td>Lecture</td>
    <td>Tuesday, Dec. 12</td>
    <td>
      Additional topics in Stanford cs231n <br>
      Societal implications of AI
    </td>
    <td>
        <a href="https://docs.google.com/presentation/d/1qTtKDajhlb5fJfXDDKNGy1C5uqOOvlNSKt4qUfZlseM/edit?usp=sharing">[slides]</a><br>
        Stanford cs231n slides not covered in our course: <a href="https://drive.google.com/file/d/0B-0OtUj_Gj7nY2toWTZIekZyRkk/view?usp=sharing">Software Packages</a>, <a href="https://drive.google.com/file/d/0B-0OtUj_Gj7nazc5ajZhZzhKTnM/view?usp=sharing">Segmentation & Attention</a>, <a href="https://drive.google.com/file/d/0B-0OtUj_Gj7nbVRkbm1tUnRUanM/view?usp=sharing">Videos & Unsupervised Learning</a>
    </td>
  </tr>
-->

</tbody></table>
</div>

    </div><!--/.container-->

    <footer class="site-footer">
      <div class="wrap">
        <div class="footer-col-1 column">
          <ul>
            <li><a href="https://github.com/compsci682-fa18">
              <span class="icon github">
                <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                   viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                  <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                  c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                  c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                  c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                  C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                  c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                  c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                  c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                  c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>
              <span class="username">compsci682-fa18</span>
            </a>
            </li>
            <li>
              <a href="mailto:compsci682fall18@gmail.com">compsci682fall18@gmail.com</a>
            </li>
          </ul>
        </div>
        <div class="footer-col-2 column">

        </div>

        <div class="footer-col-3 column">

        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="assets/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="assets/js/ie10-viewport-bug-workaround.js"></script>
    <script src="assets/js/offcanvas.js"></script>
  </body>
</html>
